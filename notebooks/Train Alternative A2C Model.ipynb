{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative model - that may be able to score better would be if we would apply a custom policy and value function.\n",
    "In the approach we take more control on how the model interpretes the environment and comes with a policy or values a certain state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "# Find the root directory by traversing up the directory tree\n",
    "def find_project_root(current_path):\n",
    "    if os.path.exists(os.path.join(current_path, \"README.md\")):\n",
    "        return current_path\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    if parent_path == current_path:\n",
    "        raise ValueError(\"Project root not found.\")\n",
    "    return find_project_root(parent_path)\n",
    "\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "project_root = find_project_root(current_dir)\n",
    "sys.path.append(str(project_root))\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# flake8: noqa  # Disable Flake8 for the following block\n",
    "import gymnasium as gym\n",
    "from src.models.bank_env import BankEnv\n",
    "from src.models.bank_model import Bankmodel\n",
    "from src.visualization import visualize\n",
    "import src.models.train as train\n",
    "from src.tests import test_bankmodel_a2c_train as tests\n",
    "from src.data.definitions import MODEL_PATH, TENSORBOARD_LOGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BasePolicy' from 'stable_baselines3.common.torch_layers' (f:\\OneDrive\\Documents\\GitHub\\rl-alm\\rl-alm\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mf:\\OneDrive\\Documents\\GitHub\\rl-alm\\notebooks\\Train Alternative A2C Model.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/OneDrive/Documents/GitHub/rl-alm/notebooks/Train%20Alternative%20A2C%20Model.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines3\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch_layers\u001b[39;00m \u001b[39mimport\u001b[39;00m BasePolicy, FlattenExtractor, MlpExtractor\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/OneDrive/Documents/GitHub/rl-alm/notebooks/Train%20Alternative%20A2C%20Model.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/OneDrive/Documents/GitHub/rl-alm/notebooks/Train%20Alternative%20A2C%20Model.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCustomPolicy\u001b[39;00m(BasePolicy):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'BasePolicy' from 'stable_baselines3.common.torch_layers' (f:\\OneDrive\\Documents\\GitHub\\rl-alm\\rl-alm\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py)"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.torch_layers import BasePolicy, FlattenExtractor, MlpExtractor\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomPolicy(BasePolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomPolicy, self).__init__(*args, **kwargs, net_arch=[256, 256])  \n",
    "        self.policy_net = nn.Sequential(\n",
    "            FlattenExtractor(self.observation_space),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, self.action_space.n),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs, deterministic=False):\n",
    "        return super().forward(obs, deterministic=deterministic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_id, render_mode=\"human\")\n",
    "\n",
    "# Define a Lineair Learning Rate Scheduler\n",
    "def linear_schedule(initial_value: float):\n",
    "    \"\"\"Linear learning rate schedule.\"\"\"\n",
    "\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"Progress will decrease from 1 (beginning) to 0.\"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "    return func\n",
    "\n",
    "initial_lr = 0.001\n",
    "n_steps = 60\n",
    "model_name = 'A2C_Alt'\n",
    "ent_coef= 0.001\n",
    "model = A2C(\n",
    "        \"custom_policy\",        \n",
    "        env,\n",
    "        device=\"cpu\",\n",
    "        tensorboard_log=TENSORBOARD_LOGS,\n",
    "        verbose=0,\n",
    "        n_steps=n_steps,\n",
    "        learning_rate=linear_schedule(initial_lr),\n",
    "        ent_coef=ent_coef,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 300_000\n",
    "\n",
    "model, mean, episode_iqr, episode_cvar, episode_rewards = train.train( model, env, total_timesteps=steps, conf_level=0.95, tb_log_name= TENSORBOARD_LOGS)\n",
    "modelpath = Path(\n",
    "    \n",
    "        MODEL_PATH,\n",
    "        model_name + \"_\" + str(steps) + \".zip\",\n",
    "    )\n",
    "model.save(modelpath)\n",
    "\n",
    "print(f\"Model name: {model_name}, Steps: {steps}, Mean: {mean}, IQR: {episode_iqr}, CVaR: {episode_cvar}\")\n",
    "visualize.plot_rewards(episode_rewards, interpolate_line=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
