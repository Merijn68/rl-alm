{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test customized gym environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register\n",
    "import numpy as np\n",
    "import random\n",
    "import stable_baselines3 as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28.1\n",
      "2.0.0a5\n"
     ]
    }
   ],
   "source": [
    "print(gym.__version__)\n",
    "print(sb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"gym_basic:basic-v2\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "max_steps_per_episode = 10 # but it won't go higher than 1\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "\n",
    "exploration_decay_rate = 0.01 #if we decrease it, will learn slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Average  reward per thousand episodes **********\n",
      "\n",
      "100 :  -0.04\n",
      "200 :  0.7000000000000004\n",
      "300 :  0.8400000000000005\n",
      "400 :  0.8600000000000005\n",
      "500 :  1.0000000000000007\n",
      "600 :  0.9600000000000006\n",
      "700 :  1.0000000000000007\n",
      "800 :  0.9800000000000006\n",
      "900 :  0.9800000000000006\n",
      "1000 :  0.9800000000000006\n",
      "\n",
      "\n",
      "********** Q-table **********\n",
      "\n",
      "[[-0.79410887 -0.9282102   1.         -0.9282102  -0.90152291]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes = []\n",
    "\n",
    "# Q-Learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    \n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        # Exploration -exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if exploration_rate_threshold > exploration_rate: \n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Update Q-table for Q(s,a)\n",
    "        q_table[state, action] = (1 - learning_rate) * q_table[state, action] + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state,:]))\n",
    "            \n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if terminated == True | truncated == True: \n",
    "            break\n",
    "            \n",
    "    # Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "    \n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    \n",
    "# Calculate and print the average reward per 10 episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes / 100)\n",
    "count = 100\n",
    "print(\"********** Average  reward per thousand episodes **********\\n\")\n",
    "\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r / 100)))\n",
    "    count += 100\n",
    "    \n",
    "# Print updated Q-table\n",
    "print(\"\\n\\n********** Q-table **********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | -0.68    |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | -0.12      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 178        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 22         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11111563 |\n",
      "|    clip_fraction        | 0.848      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.5       |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.211      |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.142     |\n",
      "|    value_loss           | 0.617      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.52       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 178        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 34         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15612891 |\n",
      "|    clip_fraction        | 0.966      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.06      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.293      |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.191     |\n",
      "|    value_loss           | 0.996      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1         |\n",
      "|    ep_rew_mean          | 0.84      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 4         |\n",
      "|    time_elapsed         | 46        |\n",
      "|    total_timesteps      | 8192      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2230039 |\n",
      "|    clip_fraction        | 0.968     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.404    |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.237     |\n",
      "|    n_updates            | 30        |\n",
      "|    policy_gradient_loss | -0.178    |\n",
      "|    value_loss           | 0.86      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.98       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 58         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.21336386 |\n",
      "|    clip_fraction        | 0.0827     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0593    |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0694     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0723    |\n",
      "|    value_loss           | 0.319      |\n",
      "----------------------------------------\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "# from stable_baselines3.td3 import MlpPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "obs = env.reset()[0]\n",
    "for i in range(10):\n",
    "    action, _states = model.predict(obs)\n",
    "    print(action)\n",
    "    obs, rewards, terminated, truncated, info = env.step(action)\n",
    "    env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlalm",
   "language": "python",
   "name": "rlalm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
